{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aea449e-fcdd-43b2-8b1e-37ea5d013384",
   "metadata": {},
   "source": [
    "# Exploring Temporal GNN Embeddings for Darknet Traffic Analysis\n",
    "## Embeddings Generation\n",
    "___\n",
    "\n",
    "## Table of Contents\n",
    "1. NLP Embeddings\n",
    "2. GNN embeddings Without Features\n",
    "3. GNN embeddings With Features\n",
    "\n",
    "This notebook contains the main codes to (i) prduce NLP embeddings through i-DarkVec; (ii) prduce (t)GNN embeddings without node features; (iii) prduce (t)GNN embeddings with node features.\n",
    "\n",
    "The tested GNNs are: i-GCN, GCN-GRU, i-GCN-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5cda90-d087-44db-9e0f-472ff2d7fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c28a0-8cc3-496d-8839-899784831ea2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. NLP Embeddings: i-DarkVec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b3742-ccd3-4d56-9b46-bcb1264fb7c2",
   "metadata": {},
   "source": [
    "This code demonstrates the process of training and updating word embeddings models using daily text data. The models learn and represent the meanings of words in a way that facilitates various NLP tasks and understanding textual data across different time periods.\n",
    "\n",
    "1. **Model Initialization:** The code begins by initializing a word embeddings model, referred to as `word2vec`. The model is configured with specific parameters, including a context window size (`c`), embedding dimensionality (`e`), number of training epochs (`epochs`), and a random seed (`seed`).\n",
    "\n",
    "2. **Processing Multiple Days' Data:** The code then enters a loop to process data from multiple days. This loop iterates over files located in the '../data/raw/' directory.\n",
    "\n",
    "3. **Extracting Day Information:** For each file, the code extracts the corresponding day from the filename. This day information is important for identifying and processing data from different time periods.\n",
    "\n",
    "4. **Loading Corpus Data:** Within the loop, the code loads a corpus of text data associated with the current day. The corpus data is stored in files with filenames like 'corpus_YYYYMMDD.pkl' and is read using the `pickle.load()` function.\n",
    "\n",
    "5. **Training or Updating the Model:** Depending on the specific day being processed, if the day is the first one, the initialized model (`word2vec`) is trained on the current day's corpus data. This training process helps the model learn word embeddings from scratch.\n",
    "\n",
    "    For other days, the code updates the pre-trained model (`word2vec`) using the corpus data from the current day. This updating process helps the model adapt and refine its embeddings based on new data.\n",
    "\n",
    "6. **Retrieving and Saving Embeddings:** After training or updating the model, the code retrieves the word embeddings learned by `word2vec`. These embeddings capture the semantic relationships between words in the corpus. The embeddings are then saved to a CSV file named 'idarkvec_embeddings_YYYYMMDD.csv' to be used for further natural language processing (NLP) tasks or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68cac68c-7ce2-425b-a893-999fecab4bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8def681dbf74985b8673c4c9cb0fd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.models.nlp import iWord2Vec\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "# Initialize the model\n",
    "word2vec = iWord2Vec(c=5, e=128, epochs=1, seed=15)\n",
    "\n",
    "for file in tqdm(sorted(glob(f'../data/raw/*'))):\n",
    "    # Extract day\n",
    "    day = file.split('/')[-1].replace('.csv', '').replace('raw_', '')\n",
    "    \n",
    "    # Load the corpus\n",
    "    with open(f'../data/corpus/corpus_{day}.pkl', 'rb') as file:\n",
    "        corpus = pickle.load(file)\n",
    "    \n",
    "    if day == '20211201':\n",
    "        # Train the initialized model\n",
    "        word2vec.train(corpus)\n",
    "    else:\n",
    "        # Update the pre-trained model\n",
    "        word2vec.update(corpus)\n",
    "    \n",
    "    # Retrieve the embeddings and save them\n",
    "    embeddings = word2vec.get_embeddings()\n",
    "    embeddings.to_csv(f'../data/nlp_embeddings/idarkvec_embeddings_{day}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8088c2-113a-43db-aaef-131ce56e5219",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. GNN Embeddings Without Node Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f40be-29a0-490c-aa60-fde99f2b7429",
   "metadata": {},
   "source": [
    "This code segment focuses on essential data loading and preprocessing steps for graph analysis, including host node mapping, feature conversion, and adjacency matrix generation.\n",
    "\n",
    "1. **Loading Host Node Lookup:** The code loads a lookup dictionary for host nodes from a JSON file. This dictionary maps IP addresses to nodes in a graph.\n",
    "\n",
    "2. **Loading and Converting Traffic Features:** A list of traffic feature files is loaded and converted into PyTorch tensors for analysis. **Note** Here they are unused\n",
    "\n",
    "3. **Generating Weighted Adjacency Matrices:** Weighted adjacency matrices are generated from graph data files. These matrices represent connections between nodes in the network.\n",
    "\n",
    "4. **Obtaining Node and Feature Counts:** The code determines the number of nodes and features in the data, which is essential for graph analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760a8558-ff4f-4f55-b188-c4a7bcaca0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac71c2240ba4a5d81a3bf776a0f19bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading features:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bda83a3643540e3901e90b2ceaf95e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading graphs:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1f422e4ac24592a7470af8c08ac1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating matrices:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.preprocessing.gnn import generate_adjacency_matrices\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "# Number of days to use as history for tGNN\n",
    "HISTORY = 5\n",
    "# Number of training days for GCN-GRU\n",
    "TRAIN = 20\n",
    "\n",
    "# Load saved lookup dictionaries for host nodes\n",
    "with open(f'../data/graph/ip_lookup.json', 'r') as file:\n",
    "    ip_lookup = json.loads(file.read())\n",
    "    reverse_lookup = {v:k for k,v in ip_lookup.items()}\n",
    "    ip_nodes = len(reverse_lookup)\n",
    "\n",
    "# Load traffic features and convert them as torch tensors\n",
    "flist = [x for x in sorted(glob(f'../data/features/*'))]\n",
    "features = []\n",
    "for file in tqdm(flist, desc='Loading features'):\n",
    "    feat = pd.read_csv(file, index_col=[0]).sort_index()\n",
    "    feat = torch.tensor(feat.to_numpy())\n",
    "    features.append(feat)\n",
    "\n",
    "    \n",
    "# Generate the weighted adjacency matrices\n",
    "flist = [x for x in sorted(glob(f'../data/graph/*')) if '.txt' in x]\n",
    "X = generate_adjacency_matrices(flist, weighted=True)\n",
    "\n",
    "# Get number of nodes and features\n",
    "n_nodes, n_features = features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d6683-6737-4f26-a187-67b504496aa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### i-GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5108c39-3853-45b9-8356-036a2e5e2843",
   "metadata": {},
   "source": [
    "This code segment focuses on training a Graph Neural Network model and extracting embeddings from snapshots of data. The embeddings capture important features of the graph structure and can be utilized for downstream tasks or analysis.\n",
    "\n",
    "\n",
    "1. **GNN Model Configuration:** A GNN model (`igcn`) is configured with various parameters, including the number of nodes (`n_nodes`), GCN layers, input and output sizes, embedding size, and other hyperparameters for training.\n",
    "\n",
    "2. **Training Loop:** The code enters a loop to iterate through each snapshot in `X`. For each snapshot the GNN model is trained or updated using the snapshot data. The embeddings are retrieved from the trained model, the index of the embeddings is adjusted to match IP nodes and the embeddings are saved to a CSV file with a name indicating the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f085f61-21f0-4e03-8996-d82aaa0d0f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from src.models.gnn import GCN\n",
    "\n",
    "epochs=5 # Set training epochs\n",
    "pbar = tqdm(total=len(X)*epochs) # Initialize progress bar\n",
    "\n",
    "# Initialize the model\n",
    "igcn = GCN(\n",
    "    n_nodes=n_nodes, \n",
    "    ns=1, \n",
    "    gcn_layers=2, \n",
    "    input_size=n_nodes, \n",
    "    gcn_units=1024, \n",
    "    gcn_output_size=512,\n",
    "    embedding_size=128, \n",
    "    predictor_units=64, \n",
    "    dropout=.0, \n",
    "    lr=1e-3, \n",
    "    cuda=False, \n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Incremental training\n",
    "for i in range(len(X)):\n",
    "    # Get current snapshot\n",
    "    X_train = X[i]\n",
    "    \n",
    "    # Train/Update the model\n",
    "    igcn.fit(X_train, pbar=pbar, day=i)\n",
    "\n",
    "    # Retrieve embeddings\n",
    "    embeddings = igcn.get_embeddings(X_train)[:ip_nodes]\n",
    "\n",
    "    # Adjust index with IP nodes\n",
    "    new_index = [reverse_lookup[x] for x in range(ip_nodes)]\n",
    "    embeddings = pd.DataFrame(embeddings, index=new_index)\n",
    "    \n",
    "    # Manage day name for saving\n",
    "    if i+1<10: day = f'0{i+1}'\n",
    "    else: day=i+1\n",
    "        \n",
    "    # Save the embeddings\n",
    "    ename = f'../data/gnn_embeddings/embeddings_igcn_202112{day}.csv'\n",
    "    embeddings.to_csv(ename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed444be3-3e3c-47f9-bb02-8c2cd8d1a081",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GCN-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37433dd5-8637-4d62-bcf7-e3c9212ed892",
   "metadata": {},
   "source": [
    "This code segment focuses on training a GNN-GRU model for graph data and extracting embeddings from sequential snapshots, which can be utilized for various downstream tasks or analysis.\n",
    "\n",
    "1. **Model Initialization:** The GNN model with a Gated Recurrent Unit (GRU), named `gcngru`, is initialized. It is configured with various parameters, including the number of nodes (`n_nodes`), history length (`HISTORY`), GCN layers, input and output sizes, embedding size, and training hyperparameters.\n",
    "\n",
    "2. **Standard Model Training:** The model is trained in a standard way using the initial snapshot data (`X[:TRAIN]`) with a specified number of epochs.\n",
    "\n",
    "3. **Embeddings Extraction:** The code enters a loop to iterate through snapshots from `TRAIN` to the end of the data. For each snapshot the code retrieves historical snapshots (`x`) from the previous `HISTORY` snapshots to the current snapshot. Embeddings are obtained using the trained GNN-GRU model, the index of the embeddings is adjusted to match IP nodes and the embeddings are saved to a CSV file with a name indicating the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39032050-db83-4b49-b139-1f50186e1748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from src.models.gnn import GCN_GRU\n",
    "\n",
    "epochs=50 # Set training epochs\n",
    "\n",
    "# Initialize the model\n",
    "gcngru = GCN_GRU(\n",
    "    n_nodes=n_nodes, \n",
    "    history=HISTORY, \n",
    "    ns=1, \n",
    "    gcn_layers=2, \n",
    "    input_size=n_nodes, \n",
    "    gcn_units=1024, \n",
    "    gcn_output_size=512, \n",
    "    embedding_size=128, \n",
    "    predictor_units=64, \n",
    "    dropout=.0, \n",
    "    lr=1e-3, \n",
    "    early_stop=3,\n",
    "    cuda=False\n",
    ")\n",
    "\n",
    "# Train the model in a standard way\n",
    "gcngru.fit(X[:TRAIN], epochs=epochs)\n",
    "\n",
    "for idx in tqdm(range(TRAIN, len(X))):\n",
    "    day=idx+1\n",
    "    # Get current snapshot\n",
    "    x = X[idx-HISTORY:idx]\n",
    "    \n",
    "    # Retrieve embeddings\n",
    "    embeddings = gcngru.get_embeddings(x)[:ip_nodes]\n",
    "    \n",
    "    # Adjust index with IP nodes\n",
    "    new_index = [reverse_lookup[x] for x in range(ip_nodes)]\n",
    "    embeddings = pd.DataFrame(embeddings, index=new_index)\n",
    "    \n",
    "    # Save the embeddings\n",
    "    embeddings.to_csv(f'../data/gnn_embeddings/embeddings_gcngru_202112{day}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8e55d-1cba-4659-a106-134cc30d2cd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### i-GCN-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653461f-cd33-41b9-9e1e-66ecb90ac204",
   "metadata": {},
   "source": [
    "This code segment demonstrates the process of incrementally training a GNN-GRU model on sequential data, extracting embeddings for each snapshot, and storing them for potential downstream tasks or analysis.\n",
    "\n",
    "1. **Model Initialization:** The GNN model with a Gated Recurrent Unit (GRU), named `igcngru`, is initialized. It is configured with various parameters, including the number of nodes (`n_nodes`), history length (`HISTORY`), GCN layers, input and output sizes, embedding size, and training hyperparameters.\n",
    "\n",
    "2. **Incremental Training Loop:** The code enters a loop to iteratively perform incremental training of the GNN-GRU model. For each iteration the current snapshot and historical snapshots are selected for training (`X_train`). The model is trained or updated using the selected data. and the embeddings are retrieved from the trained GNN-GRU model for the selected snapshots. Then, the index of the embeddings is adjusted to match IP nodes and the embeddings are saved to a CSV file with a name indicating the day of the snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e00a97-e015-4101-8a23-f66b90b38dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from src.models.gnn import IncrementalGcnGru\n",
    "\n",
    "epochs=5 # Set training epochs\n",
    "pbar = tqdm(total=(len(X)-HISTORY)*epochs) # Initialize progress bar\n",
    "\n",
    "# Initialize the model\n",
    "igcngru = IncrementalGcnGru(\n",
    "    n_nodes=n_nodes, \n",
    "    history=HISTORY, \n",
    "    ns=1, \n",
    "    gcn_layers=2, \n",
    "    input_size=n_nodes, \n",
    "    gcn_units=1024, \n",
    "    gcn_output_size=512, \n",
    "    embedding_size=128, \n",
    "    predictor_units=64, \n",
    "    dropout=.0, \n",
    "    lr=1e-3, \n",
    "    cuda=False, \n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Incremental training\n",
    "for i in range(len(X)-HISTORY):\n",
    "    # Get current snapshot\n",
    "    X_train = X[i:HISTORY+1+i]\n",
    "    \n",
    "    # Train/Update the model\n",
    "    igcngru.fit(X_train, pbar=pbar, day=i)\n",
    "\n",
    "    # Retrieve embeddings\n",
    "    embeddings = igcngru.get_embeddings(X_train)[:ip_nodes]\n",
    "\n",
    "    # Adjust index with IP nodes\n",
    "    new_index = [reverse_lookup[x] for x in range(ip_nodes)]\n",
    "    embeddings = pd.DataFrame(embeddings, index=new_index)\n",
    "    \n",
    "    # Manage day name for saving\n",
    "    if HISTORY+1+i<10: day = f'0{HISTORY+1+i}'\n",
    "    else: day=HISTORY+1+i\n",
    "        \n",
    "    # Save the embeddings\n",
    "    ename = f'../data/gnn_embeddings/embeddings_igcngru_202112{day}.csv'\n",
    "    embeddings.to_csv(ename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e38071-b726-4e02-a137-b1b036396ec4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. GNN Embeddings With Node Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa4d06-bdc4-41ee-9d27-8b50285e15ff",
   "metadata": {},
   "source": [
    "This code segment focuses on essential data loading and preprocessing steps for graph analysis, including host node mapping, feature conversion, and adjacency matrix generation.\n",
    "\n",
    "1. **Loading Host Node Lookup:** The code loads a lookup dictionary for host nodes from a JSON file. This dictionary maps IP addresses to nodes in a graph.\n",
    "\n",
    "2. **Loading and Converting Traffic Features:** A list of traffic feature files is loaded and converted into PyTorch tensors for analysis.\n",
    "\n",
    "3. **Generating Weighted Adjacency Matrices:** Weighted adjacency matrices are generated from graph data files. These matrices represent connections between nodes in the network.\n",
    "\n",
    "4. **Obtaining Node and Feature Counts:** The code determines the number of nodes and features in the data, which is essential for graph analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ec3bc-cb1b-452c-ae48-d70984a6b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.gnn import generate_adjacency_matrices\n",
    "from src.models.gnn import GCN_GRU\n",
    "from src.models.gnn import IncrementalGcnGru\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Number of days to use as history for tGNN\n",
    "HISTORY = 5\n",
    "# Number of training days for GCN-GRU\n",
    "TRAIN = 20\n",
    "\n",
    "# Load saved lookup dictionaries for host nodes\n",
    "with open(f'data/{DSET_TYPE}/graph/ip_lookup.json', 'r') as file:\n",
    "    ip_lookup = json.loads(file.read())\n",
    "    reverse_lookup = {v:k for k,v in ip_lookup.items()}\n",
    "    ip_nodes = len(reverse_lookup)\n",
    "\n",
    "# Load traffic features and convert them as torch tensors\n",
    "flist = [x for x in sorted(glob(f'data/{DSET_TYPE}/features/*'))]\n",
    "features = []\n",
    "for file in tqdm(flist):\n",
    "    feat = pd.read_csv(file, index_col=[0]).sort_index()\n",
    "    feat = torch.tensor(feat.to_numpy())\n",
    "    features.append(feat)\n",
    "\n",
    "    \n",
    "# Generate the weighted adjacency matrices\n",
    "flist = [x for x in sorted(glob(f'data/{DSET_TYPE}/graph/*')) if '.txt' in x]\n",
    "X = generate_adjacency_matrices(flist, weighted=True)\n",
    "\n",
    "# Get number of nodes and features\n",
    "n_nodes, n_features = features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f58cf0-c98b-4e5f-aae3-5c7a19fadaaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### i-GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db36be0-4353-469e-9894-038d001f87a0",
   "metadata": {},
   "source": [
    "This code segment focuses on training a Graph Neural Network model and extracting embeddings from snapshots of data. The embeddings capture important features of the graph structure and can be utilized for downstream tasks or analysis.\n",
    "\n",
    "\n",
    "1. **GNN Model Configuration:** A GNN model (`igcn`) is configured with various parameters, including the number of nodes (`n_nodes`), GCN layers, input and output sizes, embedding size, and other hyperparameters for training.\n",
    "\n",
    "2. **Training Loop:** The code enters a loop to iterate through each snapshot in `X`. For each snapshot the GNN model is trained or updated using the snapshot data. The embeddings are retrieved from the trained model, the index of the embeddings is adjusted to match IP nodes and the embeddings are saved to a CSV file with a name indicating the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c4e49-b776-47db-ab5f-fb5c1e7436fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from src.models.gnn import GCN\n",
    "\n",
    "epochs=1 # Set training epochs\n",
    "pbar = tqdm(total=len(X)*epochs) # Initialize progress bar\n",
    "\n",
    "# Initialize the model\n",
    "igcn = GCN(\n",
    "    n_nodes=n_nodes, \n",
    "    ns=1, \n",
    "    gcn_layers=2, \n",
    "    input_size=n_features, \n",
    "    gcn_units=1024, \n",
    "    gcn_output_size=512,\n",
    "    embedding_size=128, \n",
    "    predictor_units=64, \n",
    "    dropout=.0, \n",
    "    lr=1e-3, \n",
    "    cuda=False, \n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Incremental training\n",
    "for i in range(len(X)):\n",
    "    # Get current snapshot\n",
    "    X_train, features_train = X[i], features[i]\n",
    "    \n",
    "    # Train/Update the model\n",
    "    igcn.fit(X_train, features=features_train, pbar=pbar, day=i)\n",
    "\n",
    "    # Retrieve embeddings\n",
    "    embeddings = igcn.get_embeddings(X_train, features_train)[:ip_nodes]\n",
    "\n",
    "    # Adjust index with IP nodes\n",
    "    new_index = [reverse_lookup[x] for x in range(ip_nodes)]\n",
    "    embeddings = pd.DataFrame(embeddings, index=new_index)\n",
    "    \n",
    "    # Manage day name for saving\n",
    "    if i+1<10: day = f'0{i+1}'\n",
    "    else: day=i+1\n",
    "        \n",
    "    # Save the embeddings\n",
    "    ename = f'../data/gnn_embeddings/embeddings_igcn_features_202112{day}.csv'\n",
    "    embeddings.to_csv(ename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4a31b-c5f2-4fa1-8144-f9ff27f2dea7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GCN-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb01cda-ab77-4040-a856-7b91a646fe8b",
   "metadata": {},
   "source": [
    "This code segment focuses on training a GNN-GRU model for graph data and extracting embeddings from sequential snapshots, which can be utilized for various downstream tasks or analysis.\n",
    "\n",
    "1. **Model Initialization:** The GNN model with a Gated Recurrent Unit (GRU), named `gcngru`, is initialized. It is configured with various parameters, including the number of nodes (`n_nodes`), history length (`HISTORY`), GCN layers, input and output sizes, embedding size, and training hyperparameters.\n",
    "\n",
    "2. **Standard Model Training:** The model is trained in a standard way using the initial snapshot data (`X[:TRAIN]`) and node features (`features[:TRAIN]`) with a specified number of epochs.\n",
    "\n",
    "3. **Embeddings Extraction:** The code enters a loop to iterate through snapshots from `TRAIN` to the end of the data. For each snapshot the code retrieves historical snapshots (`x`) from the previous `HISTORY` snapshots to the current snapshot. Embeddings are obtained using the trained GNN-GRU model, the index of the embeddings is adjusted to match IP nodes and the embeddings are saved to a CSV file with a name indicating the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb5756-da97-49d1-85d6-a25bb3b5363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from src.models.gnn import GCN_GRU\n",
    "\n",
    "epochs=50 # Set training epochs\n",
    "\n",
    "# Initialize the model\n",
    "gcngru = GCN_GRU(\n",
    "    n_nodes=n_nodes, \n",
    "    history=HISTORY, \n",
    "    ns=1, \n",
    "    gcn_layers=2, \n",
    "    input_size=n_features, \n",
    "    gcn_units=1024, \n",
    "    gcn_output_size=512, \n",
    "    embedding_size=128, \n",
    "    predictor_units=64, \n",
    "    dropout=.0, \n",
    "    lr=1e-3, \n",
    "    early_stop=3,\n",
    "    cuda=False\n",
    ")\n",
    "\n",
    "# Train the model in a standard way\n",
    "gcngru.fit(X[:TRAIN], epochs=50, features=features[:TRAIN])\n",
    "\n",
    "for idx in tqdm(range(TRAIN, len(X))):\n",
    "    day=idx+1\n",
    "    \n",
    "    # Get current day\n",
    "    x = X[idx-HISTORY:idx]\n",
    "    feature = features[idx-HISTORY:idx]\n",
    "    \n",
    "    # Retrieve embeddings\n",
    "    embeddings = gcngru.get_embeddings(x, feature)[:ip_nodes]\n",
    "    \n",
    "    # Adjust index with IP nodes\n",
    "    new_index = [reverse_lookup[x] for x in range(ip_nodes)]\n",
    "    embeddings = pd.DataFrame(embeddings, index=new_index)\n",
    "    \n",
    "    # Save the embeddings\n",
    "    embeddings.to_csv(f'../data/gnn_embeddings/embeddings_gcngru_features_202112{day}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e87182-c5ee-4d98-bb80-0d94bd133a4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### i-GCN-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca557e7e-9f97-4e24-ae91-1c40cd7dfa4b",
   "metadata": {},
   "source": [
    "This code segment demonstrates the process of incrementally training a GNN-GRU model on sequential data, extracting embeddings for each snapshot, and storing them for potential downstream tasks or analysis.\n",
    "\n",
    "1. **Model Initialization:** The GNN model with a Gated Recurrent Unit (GRU), named `igcngru`, is initialized. It is configured with various parameters, including the number of nodes (`n_nodes`), history length (`HISTORY`), GCN layers, input and output sizes, embedding size, and training hyperparameters.\n",
    "\n",
    "2. **Incremental Training Loop:** The code enters a loop to iteratively perform incremental training of the GNN-GRU model. For each iteration the current snapshot and historical snapshots are selected for training (`X_train`). The model is trained or updated using the selected data. and the embeddings are retrieved from the trained GNN-GRU model for the selected snapshots. Then, the index of the embeddings is adjusted to match IP nodes and the embeddings are saved to a CSV file with a name indicating the day of the snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea0377-525c-4fe5-b67c-ecc0694bacd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from src.models.gnn import IncrementalGcnGru\n",
    "\n",
    "epochs=1 # Set training epochs\n",
    "pbar = tqdm(total=(len(X)-HISTORY)*epochs) # Initialize progress bar\n",
    "\n",
    "# Initialize the model\n",
    "igcngru = IncrementalGcnGru(\n",
    "    n_nodes=n_nodes, \n",
    "    history=HISTORY, \n",
    "    ns=1, \n",
    "    gcn_layers=2, \n",
    "    input_size=n_features, \n",
    "    gcn_units=1024, \n",
    "    gcn_output_size=512, \n",
    "    embedding_size=128, \n",
    "    predictor_units=64, \n",
    "    dropout=.0, \n",
    "    lr=1e-3, \n",
    "    cuda=False, \n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Incremental training\n",
    "for i in range(len(X)-HISTORY):\n",
    "    # Get current snapshot\n",
    "    X_train, features_train = X[i:HISTORY+1+i], features[i:HISTORY+1+i]\n",
    "\n",
    "    # Train/Update the model\n",
    "    igcngru.fit(X_train, features=features_train, pbar=pbar, day=i)\n",
    "\n",
    "    # Retrieve embeddings\n",
    "    embeddings = igcngru.get_embeddings(X_train, features_train)[:ip_nodes]\n",
    "\n",
    "    # Adjust index with IP nodes\n",
    "    new_index = [reverse_lookup[x] for x in range(ip_nodes)]\n",
    "    embeddings = pd.DataFrame(embeddings, index=new_index)\n",
    "\n",
    "    # Manage day name for saving\n",
    "    if HISTORY+1+i<10: day = f'0{HISTORY+1+i}'\n",
    "    else: day=HISTORY+1+i\n",
    "        \n",
    "    # Save the embeddings\n",
    "    ename = f'../data/gnn_embeddings/embeddings_igcngru_features_202112{day}.csv'\n",
    "    embeddings.to_csv(ename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
