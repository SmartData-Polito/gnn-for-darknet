{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a4738dc-44d9-41b8-88bd-e187732fe688",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploring Temporal GNN Embeddings for Darknet Traffic Analysis\n",
    "## Dataset Characterization\n",
    "___\n",
    "\n",
    "## Table of Contents\n",
    "1. Loading and Processing Raw Trace Data\n",
    "2. Total statistics\n",
    "3. Average Daily Statistics\n",
    "4. Graph Statistics\n",
    "\n",
    "This notebook contains the main codes to characterized both the filtered dataset (total and on daily basis) and the resulting temporal graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52ec6d94-69f2-4cc3-bd25-c825afae28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.preprocessing import *\n",
    "\n",
    "# Total snapshots of the collection\n",
    "TOT_DAYS = 31 \n",
    "# Drop source hosts sending less than FILTER packets per snapshot\n",
    "FILTER = 5 \n",
    "# Generate the corpora keeping the top TOP_PORTS daily ports +1 as languages\n",
    "TOP_PORTS = 2500 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a16e68-9c24-4ca7-a6ad-ec9d11fdd811",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Loading and Processing Raw Trace Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8ba2e-1bd0-4e7c-84ea-de0c3c9f740d",
   "metadata": {},
   "source": [
    "This code loads, processes, and stores raw trace data for multiple days, with each day's data organized by hour and aggregated into a daily dataframe. Filters can be applied to refine the data further before storage.\n",
    "\n",
    "1. **Load Raw Trace Files:** The code retrieves a list of raw trace files using the `glob` function.\n",
    "\n",
    "2. **Process Hourly Traces:** The code iterates through the hourly trace files for the current day. These files are sorted in chronological order. For each hourly trace file:\n",
    "     - The code extracts the day from the filename, indicating the date of the data being processed.\n",
    "     - The `load_single_file` function is called to load the trace data from the current hourly file for the specified day.\n",
    "     - The loaded dataframe is appended to the `dfs` list, allowing data to be collected for all hours of the current day.\n",
    "     - This step ensures that data is organized and aggregated by day and hour.\n",
    "\n",
    "3. **Daily Dataframe:** After processing all hourly files for the current day, the code concatenates the dataframes in the `dfs` list along the rows axis (`axis=0`) to create a daily dataframe, denoted as `df`. This daily dataframe contains data for the entire day, with data from each hour stacked on top of one another.\n",
    "\n",
    "4. **Packets Filtering:** The code applies a packet filter (`FILTER`) to the daily dataframe `df` to further refine the data, potentially removing or selecting specific packets based on the filter criteria.\n",
    "\n",
    "5. **Data Storage:** The processed daily dataframe (`df`) is appended to the `processed_df` list. This step ensures that data for each day is stored for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "763d719c-4c13-42a7-9a39-0bc93137ad11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef23b1ce6ab49f0b17009ea174ef5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Setting up:   0%|          | 0/775 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load raw trace files list\n",
    "flist = glob(f'../data/traces/*.log.gz')\n",
    "\n",
    "# Initialize progress bar\n",
    "pbar = tqdm(total=len(flist)+TOT_DAYS, desc='Setting up')\n",
    "\n",
    "processed_df = []\n",
    "for i in range(TOT_DAYS):\n",
    "    dfs = []\n",
    "    for file in sorted(flist)[24*i:24*(i+1)]:\n",
    "        # Retrieve current day traces\n",
    "        day = file.split('trace_')[-1][:8]\n",
    "        df = load_single_file(file, day)\n",
    "        dfs.append(df)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_description(f'{day} Loading raw traces')\n",
    "        pbar.update(1)\n",
    "        \n",
    "    # Get daily dataframe\n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Packets filter\n",
    "    pbar.set_description(f'{day} Running packets filter')\n",
    "    df = apply_packets_filter(df, FILTER)\n",
    "    \n",
    "    processed_df.append(df)\n",
    "        \n",
    "    # Update progress bar\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b7f84-fc8b-4573-908e-86b5a6893ba1",
   "metadata": {},
   "source": [
    "## 2. Total statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee768c0-8e92-4359-97d0-42a3069639ee",
   "metadata": {},
   "source": [
    "The following code generate label-specific statistics for all the snapshots in the collection. Namely, it extracts (i) number of hosts, (ii) number of contacted ports and (iii) total sent packets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "621d6626-ca64-47b8-9b4d-bb090124e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: (60106, (62030013, 3))\n",
      "                 src_ip  dst_port      pkts\n",
      "label                                      \n",
      "mirai             16147      2094   1982205\n",
      "unk_bruteforcer     976      9791  10191173\n",
      "unk_spammer        1014     49783   4891353\n",
      "shadowserver        289        42    218443\n",
      "driftnet            252      9246    564854\n",
      "internetcensus      271       252    213909\n",
      "censys              329     65069   3400900\n",
      "rapid7              344       139     60469\n",
      "onyphe              115       186     39030\n",
      "netsystems           45       199    226559\n",
      "shodan               36      1232    320861\n",
      "unk_exploiter       430        33    148210\n",
      "securitytrails       18       207    107826\n",
      "intrinsec            12         8      9403\n",
      "src_ip         20278\n",
      "dst_port      138281\n",
      "pkts        22375195\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# List of labels to keep for data filtering\n",
    "to_keep = [\n",
    "    'mirai', 'unk_bruteforcer', 'unk_spammer', 'shadowserver', \n",
    "    'driftnet', 'internetcensus', 'censys', 'rapid7', 'onyphe', \n",
    "    'netsystems', 'shodan', 'unk_exploiter', 'securitytrails', \n",
    "    'intrinsec']\n",
    "\n",
    "# Load ground truth\n",
    "gt = pd.read_csv(f'../data/ground_truth/ground_truth.csv')\n",
    "\n",
    "# Concatenate processed dataframes, select relevant columns, and add a 'pkts' column\n",
    "maindf = pd.concat(processed_df, axis=0, ignore_index=True)[['src_ip', 'dst_port']]\n",
    "maindf['pkts'] = 1\n",
    "print(f'Total: {maindf.src_ip.unique().shape[0], maindf.shape}')\n",
    "\n",
    "# Merge ground truth data with the main dataframe and drop rows with missing values\n",
    "maindf = maindf.merge(gt, on='src_ip', how='left').dropna()\n",
    "\n",
    "# Group data by label and perform aggregations\n",
    "print(maindf.groupby('label').agg({\n",
    "    'src_ip':lambda x: len(set(x)),\n",
    "    'dst_port':lambda x:len(set(x)),\n",
    "    'pkts':sum\n",
    "}).loc[to_keep])\n",
    "\n",
    "# Calculate the sum of aggregated data for the selected labels\n",
    "print(maindf.groupby('label').agg({\n",
    "    'src_ip':lambda x: len(set(x)),\n",
    "    'dst_port':lambda x:len(set(x)),\n",
    "    'pkts':sum\n",
    "}).loc[to_keep].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeef6a4-48ab-48d0-b09d-3b056d385043",
   "metadata": {},
   "source": [
    "## 3. Average Daily Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898f192-1f30-478a-9af2-ce7f67b87159",
   "metadata": {},
   "source": [
    "The following code generate label-specific statistics on average over the snapshots in the collection. Namely, it extracts (i) number of hosts, (ii) number of contacted ports and (iii) total sent packets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "136b377a-1a7c-42d4-9d50-38fc5886bc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 src_ip  dst_port      pkts\n",
      "label                                      \n",
      "mirai             16147      2094   1982205\n",
      "unk_bruteforcer     976      9791  10191173\n",
      "unk_spammer        1014     49783   4891353\n",
      "shadowserver        289        42    218443\n",
      "driftnet            252      9246    564854\n",
      "internetcensus      271       252    213909\n",
      "censys              329     65069   3400900\n",
      "rapid7              344       139     60469\n",
      "onyphe              115       186     39030\n",
      "netsystems           45       199    226559\n",
      "shodan               36      1232    320861\n",
      "unk_exploiter       430        33    148210\n",
      "securitytrails       18       207    107826\n",
      "intrinsec            12         8      9403\n",
      "src_ip         39828.0\n",
      "dst_port       66028.0\n",
      "pkts        39654818.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# List of labels to keep for data filtering\n",
    "to_keep = [\n",
    "    'mirai', 'unk_bruteforcer', 'unk_spammer', 'shadowserver', \n",
    "    'driftnet', 'internetcensus', 'censys', 'rapid7', 'onyphe', \n",
    "    'netsystems', 'shodan', 'unk_exploiter', 'securitytrails', \n",
    "    'intrinsec']\n",
    "\n",
    "# Load ground truth\n",
    "gt = pd.read_csv(f'../data/ground_truth/ground_truth.csv')\n",
    "\n",
    "# Concatenate processed dataframes, select relevant columns, and add a 'pkts' column\n",
    "maindf = pd.concat(processed_df, axis=0, ignore_index=True)[['src_ip', 'dst_port', 'interval']]\n",
    "maindf['pkts'] = 1\n",
    "\n",
    "# Merge ground truth data with the main dataframe and drop rows with missing values\n",
    "maindf1 = maindf.merge(gt, on='src_ip', how='left').dropna()\n",
    "\n",
    "# Group data by label and snapshot and perform aggregations\n",
    "tmp = maindf1.groupby(['label', 'interval']).agg({\n",
    "    'src_ip':lambda x: len(set(x)),\n",
    "    'dst_port':lambda x:len(set(x)),\n",
    "    'pkts':sum\n",
    "})\n",
    "print(tmp.reset_index()\\\n",
    "         .groupby('label')\\\n",
    "         .mean()\\\n",
    "         .loc[to_keep]\\\n",
    "         .astype(int))\n",
    "\n",
    "# Get 'Unknown' statistics\n",
    "maindf2 = maindf.merge(gt, on='src_ip', how='left').fillna('unknown')\n",
    "# Group data by snapshot only and perform aggregations\n",
    "print(maindf2[~maindf2.label.isin(to_keep)]\\\n",
    "          .groupby(['interval'])\\\n",
    "          .agg({'src_ip':lambda x: len(set(x)),\n",
    "                'dst_port':lambda x: len(set(x)),\n",
    "                'pkts':sum}).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b77dfe-11c4-471f-a2ce-8f8b2eb284d7",
   "metadata": {},
   "source": [
    "### 4. Graph Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260a59a-7d3d-4ed4-a846-b18070a2da2c",
   "metadata": {},
   "source": [
    "The following code loads, processes, and summarizes data from multiple graph snapshots, providing insights into network structures.\n",
    "\n",
    "1. **Load Snapshots:** Graph snapshot data is loaded from '.txt' files in the '../data/graph/' directory into a list named `traces`.\n",
    "\n",
    "2. **Concatenate Snapshots:** All loaded snapshots are merged into a single DataFrame, `traces`.\n",
    "\n",
    "3. **Total Edges:** The total number of edges in `traces` is computed and stored in `tot_edges`.\n",
    "\n",
    "4. **Snapshot Analysis:** For each unique 'interval' value in the DataFrame calculate the number of unique nodes and total edges in the snapshot and append node and edge counts to respective lists.\n",
    "\n",
    "5. **Summary Statistics:** Calculate and print:\n",
    "   - Total nodes (sum of all unique nodes).\n",
    "   - Total edges (stored as `tot_edges`).\n",
    "   - Average nodes (mean of node counts).\n",
    "   - Average edges (mean of edge counts, rounded to an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e5b06ca-15f3-403e-af5c-829f7e19310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 198171\n",
      "Total edges: 1525152\n",
      "Avg. nodes per snapshot: 6392\n",
      "Avg. edges per snapshot: 49198\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load generated graphs -- each file is a snapshot\n",
    "traces = []\n",
    "flist = [x for x in sorted(glob(f'../data/graph/*')) if '.txt' in x]\n",
    "for i, fname in enumerate(flist):\n",
    "    data = pd.read_csv(fname, header=None, \n",
    "                       names=[\"src\", \"dst\", \"weight\", \"label\"])\n",
    "    data['interval'] = i # Manage the snapshot info\n",
    "    traces.append(data)\n",
    "    \n",
    "# Concatenate loaded graphs\n",
    "traces = pd.concat(traces, ignore_index=True)\n",
    "\n",
    "tot_edges = traces.shape[0]\n",
    "nodes, edges = [], []\n",
    "for t in traces.interval.unique():\n",
    "    # Extract snapshots\n",
    "    trace = traces[traces.interval==t]\n",
    "    \n",
    "    # Calculate the number of distinct nodes and edges in each snapshot\n",
    "    node = np.hstack([trace.src.unique(), trace.dst.unique()]).shape[0] # Nodes\n",
    "    edge = trace.shape[0] # Edges    \n",
    "    nodes.append(node), edges.append(edge)\n",
    "    \n",
    "# Calculate and print the total number of nodes, total edges, average nodes, and average edges\n",
    "print(f'Total nodes: {np.sum(nodes)}')\n",
    "print(f'Total edges: {tot_edges}') \n",
    "print(f'Avg. nodes per snapshot: {np.mean(nodes).astype(int)}') \n",
    "print(f'Avg. edges per snapshot: {np.mean(edges).astype(int)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
